{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping stock news and analyzing sentiment\n",
    "For the moderately interested day trader and programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup #need to pip install bs4\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "# import tkinter\n",
    "# from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = ['Facebook','Alibaba']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the companies I currently have a few stocks in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting headlines and links from Google News\n",
    "The news page URL is consistent over different search terms, so I can use the `replace` function to construct a url for each company I am interested in. The Google News page also has a consistent format for every search, which tags each link/headline with the identifier \"aria-level=2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_links(company):\n",
    "    \"\"\"returns the company's news links and headlines\"\"\"\n",
    "    headlines = []\n",
    "    links = []\n",
    "\n",
    "    quote_page = \"https://news.google.com/news/search/section/q/$REPLACE/$REPLACE?hl=en&gl=US&ned=us\"\n",
    "    quote_page = quote_page.replace(\"$REPLACE\", company)\n",
    "        \n",
    "    page = urllib.request.urlopen(quote_page)\n",
    "    soup = BeautifulSoup(page, \"html.parser\") \n",
    "        \n",
    "    arias = soup.find_all('a')\n",
    "    #look for aria-level=\"2\"\n",
    "    for aria in arias:\n",
    "        if aria.get('aria-level')==\"2\":\n",
    "            if aria.get('href')[0:1]==\"h\": #if it is a https link\n",
    "                headlines.append(aria.string)\n",
    "                links.append(aria.get('href'))\n",
    "\n",
    "    links = list(dict.fromkeys(links)) #orders the links\n",
    "\n",
    "    return headlines, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the news to Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 6):\n",
    "    \"\"\"prints news every 10 minutes for one hour\"\"\"\n",
    "    #root = tkinter.Tk()\n",
    "    #root.withdraw()\n",
    "    #messagebox.showinfo(\"Alert\", \"You have new news!\")\n",
    "    for company in companies:\n",
    "        company_headlines, company_links = retrieve_links(company)\n",
    "        print(company, \"\\n\")\n",
    "        for i in range(0, 3):\n",
    "            print(company_headlines[i])\n",
    "            print(company_links[i])\n",
    "            print('\\n')\n",
    "        print('\\n')\n",
    "    time.sleep(10000)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My news automatically refreshes every 10 minutes and prints out in this notebook! (I would like to send a notification to my computer, but Python's messagebox is a little awkward and doesn't close properly when I stop the program. It's currently commented out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the news sentiment around the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk #need pip install -U nltk\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_headlines = {}\n",
    "company_links = {}\n",
    "\n",
    "for company in companies:\n",
    "    headlines, links = retrieve_links(company)\n",
    "    company_headlines[company] = headlines\n",
    "    company_links[company] = links\n",
    "\n",
    "#It's good practice to separate the retrieval from the analysis, \n",
    "#because I'm frequently going to be testing the analysis and don't\n",
    "#want to send a new request to the web server every time I change something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the text from each news article link\n",
    "I assumed all of the text would be in the < p > tags without any attributes, so I filtered for those. This generates a pretty good rendering of the article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_articles = {}\n",
    "for company in companies:\n",
    "    articles = []\n",
    "    for link in company_links[company]:\n",
    "        try:\n",
    "            page = urllib.request.urlopen(link)\n",
    "        except:\n",
    "            break; #passes over inaccessible links\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        html_text = soup.find_all('p')\n",
    "        text = \"\"\n",
    "        for tag in html_text:\n",
    "            if tag.attrs == {}:\n",
    "                text += tag.text\n",
    "        articles.append(text)\n",
    "    company_articles[company] = articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to filter out the links that generated bad text, either from paywalls or improperly formatted HTML tags. From this query it looks like the average article length is around 2400 words, so I'll say the cutoff for a good length article is 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394.0555555555557\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "count = 0\n",
    "for company in companies:\n",
    "    for article in company_articles[company]:\n",
    "        count+=1\n",
    "        length += len(article)\n",
    "print(length/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    company_articles[company] = [article for article in company_articles[company] if len(article)>=1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My articles for each company are clean and ready for analysis now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment Analysis\n",
    "Now it's time for sentiment analysis using the Vader module in NLTK. I referenced this site to build my code: http://www.nltk.org/howto/sentiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score for Facebook today: 0.87\n",
      "Sentiment score for Alibaba today: 9.88\n"
     ]
    }
   ],
   "source": [
    "for company in companies:  \n",
    "    negativity = 0\n",
    "    positivity = 0\n",
    "    for article in company_articles[company]:\n",
    "        sentences = tokenize.sent_tokenize(article)\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        for sentence in sentences:\n",
    "            #print(sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            #for k in sorted(ss): # the dictionary categories are compound, neg, neu, and pos\n",
    "                #print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "            negativity += ss['neg']\n",
    "            positivity += ss['pos']\n",
    "    print(\"Sentiment score for\", company, \"today:\", '%.2f'%(positivity/negativity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now I have the sentiment score for every article, and I will be able to tell if people are really liking the company today (sentiment > 1.0) or hating it today (sentiment < 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
